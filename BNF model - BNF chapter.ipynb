{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pandasql import sqldf\n",
    "from scipy import stats\n",
    "import time\n",
    "import networkx as nx\n",
    "import simple_icd_10_cm as cm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_pats = 120000 # CHOOSE\n",
    "dataset_loc = \"...\" # Insert path to preprocessed csv\n",
    "\n",
    "# Additional preprocessing\n",
    "\n",
    "bnf_df2 = pd.read_csv(dataset_loc)\n",
    "bnf_list = bnf_df2[\"bnfchapter\"].value_counts()[:50].index  # List of BNF chapters to use as nodes in network\n",
    "pats = np.random.choice(bnf_df2['patid'].unique(), num_pats, replace=False) # Choose random patients to use\n",
    "bnf_df = bnf_df2[bnf_df2['patid'].isin(pats)]\n",
    "bnf_df['eventdate'] = pd.to_datetime(bnf_df['eventdate'])\n",
    "bnf_df = bnf_df.reset_index(drop=True)\n",
    "\n",
    "# Model\n",
    "\n",
    "result = []\n",
    "for bnf2 in bnf_list: # Cycle through bnf_list\n",
    "    # Dataframe for all patients and include flag if they are prescribed with D2 within 5 years\n",
    "    q = \"\"\"\n",
    "    SELECT *, event2_bnf IS NOT NULL FROM (\n",
    "    SELECT a.*, b.bnfchapter AS event2_bnf, b.eventdate as event2_date\n",
    "    FROM bnf_df AS a\n",
    "    LEFT JOIN bnf_df AS b\n",
    "    ON a.patid = b.patid\n",
    "    AND b.eventdate > a.eventdate\n",
    "    AND julianday(b.eventdate) - julianday(a.eventdate) < 1825\n",
    "    AND b.bnfchapter = {}\n",
    "    LIMIT 100000000) AS T\n",
    "    LIMIT 100000000;\n",
    "    \"\"\".format(bnf2)\n",
    "    df = sqldf(q)\n",
    "    \n",
    "    # Convert eventdate column to datetime datatype\n",
    "    df['eventdate'] = pd.to_datetime(df['eventdate'])\n",
    "    \n",
    "    for bnf1 in bnf_list:\n",
    "        if bnf1 == bnf2:\n",
    "            continue # Skip to next iteration if bnf1 == bnf2\n",
    "        \n",
    "        # Dataframe for all D1 patients, with those leading to D2 indicated\n",
    "        D1_patients = df.loc[df['bnfchapter'] == bnf1]\n",
    "\n",
    "        # Find earliest episode with D1 per patient\n",
    "        D1_patients = D1_patients.loc[D1_patients.groupby('patid').eventdate.idxmin()]\n",
    "        \n",
    "        # Total D1 patients which lead to D2\n",
    "        num_D1_to_D2 = D1_patients['event2_bnf IS NOT NULL'].sum()\n",
    "        \n",
    "        Total_D1_patients = len(D1)\n",
    "        \n",
    "        dx_pair_result = []\n",
    "        dx_pair_result.append(bnf1)\n",
    "        dx_pair_result.append(bnf2)\n",
    "        dx_pair_result.append(Total_D1_patients)\n",
    "        dx_pair_result.append(num_D1_to_D2)        \n",
    "        result.append(dx_pair_result)\n",
    "        \n",
    "MainResult = pd.DataFrame(result)\n",
    "print(\"Main model finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matched dataset\n",
    "matched_rows = []\n",
    "for row in bnf_df.itertuples(index=False, name='Pandas'):\n",
    "    try: # Find rows similar to row in original dataframe subject to conditions\n",
    "        matched_rows.append(bnf_df[\n",
    "        (row[2] - pd.Timedelta(days = 30) < bnf_df['eventdate']) &\n",
    "        (bnf_df['eventdate'] < row[2] + pd.Timedelta(days = 30)) &\n",
    "        (bnf_df['gender'] == row[4]) &\n",
    "        (row[5] - 5 < bnf_df['yob']) &\n",
    "        (bnf_df['yob'] < row[5] + 5) &\n",
    "        (bnf_df['gen_ethnicity_int'] == row[11]) &\n",
    "        (bnf_df['bnfchapter'] != row[8]) &\n",
    "        (bnf_df['patid'] != row[0])\n",
    "        ].sample(1))\n",
    "    except:\n",
    "        # If no matches, add a blank row\n",
    "        matched_rows.append(pd.DataFrame(np.array([-1,0,'2000-01-01',0,0,0,'NO MATCH',0,0,0,0,0],ndmin=2), columns = \n",
    "                                         ['patid','bnfcode', 'eventdate', 'num', 'gender', 'yob', 'gen_ethnicity', \n",
    "                                          'bnf', 'bnfchapter', 'bnfsection', 'bnfparagraph', 'gen_ethnicity_int']))\n",
    "\n",
    "# Convert to dataframe\n",
    "matched_df = pd.DataFrame(np.array(matched_rows).squeeze(), columns =\n",
    "                                         ['patid','bnfcode', 'eventdate', 'num', 'gender', 'yob', 'gen_ethnicity', \n",
    "                                          'bnf', 'bnfchapter', 'bnfsection', 'bnfparagraph', 'gen_ethnicity_int'])\n",
    "# Change column types\n",
    "matched_df = matched_df.astype({'patid': 'int64', 'bnfcode': 'int64', 'num': 'int64', 'gender': 'int64',\n",
    "             'yob': 'int64', 'bnfchapter': 'int64', 'bnfsection': 'int64',\n",
    "             'bnfparagraph': 'int64', 'gen_ethnicity_int': 'int64'})\n",
    "# Rename columns\n",
    "matched_df = matched_df.rename(columns={\"patid\": \"m_patid\", \"bnfcode\": \"m_bnfcode\", \"eventdate\": \"m_eventdate\", \"num\": \"m_num\",\n",
    "                                        \"gender\": \"m_gender\", \"yob\": \"m_yob\",\"gen_ethnicity\": \"m_gen_ethnicity\", \n",
    "                                        \"bnf\": \"m_bnf\", \"bnfchapter\": \"m_bnfchapter\", \"bnfsection\": \"m_bnfsection\",\n",
    "                          \"bnfparagraph\": \"m_bnfparagraph\", \"gen_ethnicity_int\": \"m_gen_ethnicity_int\"})\n",
    "print(\"matched df constructed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of matched patients who are prescribed with D2 within 5 years\n",
    "\n",
    "# Join original and matched dataframes\n",
    "fulldf = pd.concat([bnf_df, matched_df], axis=1, join=\"inner\")\n",
    "\n",
    "# Convert date columns to datetime datatype\n",
    "fulldf['eventdate'] = pd.to_datetime(fulldf['eventdate'])\n",
    "fulldf['m_eventdate'] = pd.to_datetime(fulldf['m_eventdate'])\n",
    "\n",
    "result = []\n",
    "for bnf2 in bnf_list: # Cycle through bnf_list\n",
    "    # Dataframe for all patients and include flag if they lead to D2\n",
    "    q = \"\"\"\n",
    "    SELECT *, event2_bnf IS NOT NULL FROM (\n",
    "    SELECT a.*, b.patid AS patid_dup, b.bnfchapter AS event2_bnf, b.eventdate as event2_date\n",
    "    FROM fulldf AS a\n",
    "    LEFT JOIN bnf_df AS b\n",
    "    ON a.m_patid = b.patid\n",
    "    AND b.eventdate > a.m_eventdate\n",
    "    AND julianday(b.eventdate) - julianday(a.m_eventdate) < 1825\n",
    "    AND b.bnfchapter = {}\n",
    "    LIMIT 100000000) AS T\n",
    "    LIMIT 100000000;\n",
    "    \"\"\".format(bnf2)\n",
    "    df = sqldf(q)\n",
    "    \n",
    "    # Convert epistart and end cols to datetime datatype\n",
    "    df['eventdate'] = pd.to_datetime(df['eventdate'])\n",
    "    \n",
    "    for bnf1 in bnf_list:\n",
    "        if bnf1 == bnf2:\n",
    "            continue # Skip to next iteration if bnf1 == bnf2\n",
    "        \n",
    "        # Dataframe for all D1 patients with their matches and those leading to D2 indicated\n",
    "        D1_patients = df.loc[df['bnfchapter'] == bnf1]\n",
    "\n",
    "        # Find earliest episode with D1 per patient\n",
    "        D1_patients = D1_patients.loc[D1_patients.groupby('patid').eventdate.idxmin()]\n",
    "        \n",
    "        # Filter out NO MATCH rows\n",
    "        D1_patients = D1_patients[D1_patients[\"m_patid\"]!=-1]\n",
    "\n",
    "        # Total D1 patients which lead to D2\n",
    "        num_D1_to_D2 = D1_patients['event2_bnf IS NOT NULL'].sum()\n",
    "        \n",
    "        dx_pair_result = []\n",
    "        dx_pair_result.append(bnf1)\n",
    "        dx_pair_result.append(bnf2)\n",
    "        D1_V2 = D1[D1['m_bnfchapter']!=bnf2] # Filter out rows where matched BNF code == D2\n",
    "        Total_D1_patients = len(D1_V2)\n",
    "        dx_pair_result.append(Total_D1_patients)\n",
    "        dx_pair_result.append(num_D1_to_D2)        \n",
    "        result.append(dx_pair_result)\n",
    "        \n",
    "# Convert results to dataframe \n",
    "MatchResult = pd.DataFrame(result)   \n",
    "MainResult = MainResult.rename(columns = {0:'D1', 1:'D2', 2:'Total_D1_patients', 3:'D1toD2'})\n",
    "MatchResult = MatchResult.rename(columns = {0:'D1', 1:'D2', 2:'matchlen', 3:'matchtoD2'})\n",
    "\n",
    "# Combine results into one df\n",
    "MainResult = pd.concat([MainResult, MatchResult['matchlen']], axis=1)\n",
    "MainResult = pd.concat([MainResult, MatchResult['matchtoD2']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate relative risk as well as RR confidence intervals\n",
    "\n",
    "def RelRisk(row):\n",
    "    if row['matchtoD2'] > 0:\n",
    "        return (row['D1toD2']/row['Total_D1_patients'])/(row['matchtoD2']/row['matchlen'])\n",
    "    else:\n",
    "        return 1\n",
    "MainResult['RR'] = MainResult.apply(lambda row: RelRisk(row), axis=1)\n",
    "\n",
    "def LogCI(row): # Calculate SE(log(RR))\n",
    "    if row['matchtoD2'] == 0 or row['D1toD2'] == 0:\n",
    "        return float(\"inf\")\n",
    "    else:\n",
    "        return np.sqrt(1/row['D1toD2']-1/row['Total_D1_patients']+1/row['matchtoD2']-1/row['matchlen'])\n",
    "MainResult['Log_CI'] = MainResult.apply(lambda row: LogCI(row), axis=1)\n",
    "\n",
    "def CImin(row): # Lower bound for CI\n",
    "    if row['RR']-4.26*row['Log_CI']>=0: # Change 4.26 to appropriate (Bonferroni-corrected) z-score\n",
    "        return np.exp(np.log(row['RR']-4.26*row['Log_CI']))\n",
    "    else:\n",
    "        return 0\n",
    "MainResult['CImin'] = MainResult.apply(lambda row: CImin(row), axis=1)\n",
    "\n",
    "def CImax(row): # Upper bound for CI\n",
    "    if row['RR']+4.26*row['Log_CI']>=0:\n",
    "        return np.exp(np.log(row['RR']+4.26*row['Log_CI']))\n",
    "    else:\n",
    "        return 10\n",
    "MainResult['CImax'] = MainResult.apply(lambda row: CImax(row), axis=1)\n",
    "\n",
    "def CIover1(row): # Indicate if CI crosses over 1\n",
    "    if row['CImin'] <= 1 and row['CImax'] <= 1:\n",
    "        return 1\n",
    "    elif row['CImin'] >= 1 and row['CImax'] >= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "MainResult['CIover1'] = MainResult.apply(lambda row: CIover1(row), axis=1)\n",
    "\n",
    "def CIsize(row):\n",
    "    if row['CImax'] == float(\"inf\"):\n",
    "        return 10\n",
    "    else:\n",
    "        return row['CImax'] - row['CImin']\n",
    "MainResult['CIsize'] = MainResult.apply(lambda row: CIsize(row), axis=1)\n",
    "\n",
    "# Save to csv\n",
    "# MainResult.to_csv('FULL models/BNF/BNFSec{}Results.csv'.format(num_pats), index=False)\n",
    "\n",
    "# Final result metrics to report\n",
    "metrics = []\n",
    "metrics.append(len(fulldf)) # Total dataset size\n",
    "metrics.append(1-np.sum(matched_df['m_patid']==-1)/len(matched_df)) # Percentage of matched patients found\n",
    "metrics.append(np.mean(MainResult['CIsize'])) # Av RR interval size\n",
    "metrics.append(np.sum(MainResult['CIover1'])/len(MainResult)) # Percentage CIs that do not cross 1\n",
    "# pd.DataFrame(metrics).to_csv('BNFSec{}Metrics.csv'.format(num_pats), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results with another network\n",
    "\n",
    "df1 = MainResult\n",
    "df2 = pd.read_csv('...') # Load another network\n",
    "# Compare adjacency matrices using various metrics\n",
    "df1 = df1.rename(columns={'RR':\"RR2\"}) # Change column name so no clash\n",
    "df2 = pd.concat([df1['RR2'], MainResult['RR']], axis=1, join=\"inner\")\n",
    "dists = []\n",
    "dists.append(np.sqrt(np.sum(df2.apply(lambda row: (row['RR2']-row['RR'])**2 ,axis=1)))) # Euclidean\n",
    "dists.append(np.sum(df2.apply(lambda row: abs(row['RR2']-row['RR']) ,axis=1))) # Manhattan\n",
    "dists.append(1-np.sum(df2.apply(lambda row: min(row['RR2'],row['RR']),axis=1))/np.sum(df2.apply(lambda row: max(row['RR2'],row['RR']),axis=1))) # Weighted Jaccard\n",
    "# pd.DataFrame(dists).to_csv('...'.format(num_pats), index=False) # Save distance metric results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
